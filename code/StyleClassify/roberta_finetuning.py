# -*- coding: utf-8 -*-
"""Copy of 1.text_classifier_SST2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15QM1N8wnEWlFktTGskZw48DVsiETrUxJ
"""

! pip install transformers
! pip install imbalanced-learn
! pip install timebudget

import pandas as pd
import numpy as np
import json, re
import uuid

from tqdm import tqdm_notebook

try:
    from collections import OrderedDict
except ImportError:
    from ordereddict import OrderedDict

# Torch, Sklearn imports
from sklearn.model_selection import train_test_split
import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
from torch.utils.data import Dataset, DataLoader, RandomSampler

## NLP libs
from nltk import download
import gensim

## PyTorch Transformer
import transformers

## Roberta
from transformers import RobertaModel, RobertaTokenizer
from transformers import RobertaForSequenceClassification, RobertaConfig

from timebudget import timebudget
timebudget.report_atexit()  # Generate report when the program exits

import warnings
warnings.filterwarnings("ignore")

stopwords = {"ourselves", "hers", "between", "yourself", "but", "again", "there", "about", "once", "during", "out", "very", "having", "with", "they", "own", "an", "be", "some", "for", "do", "its", "yours", "such", "into", "of", "most", "itself", "other", "off", "is", "s", "am", "or", "who", "as", "from", "him", "each", "the", "themselves", "until", "below", "are", "we", "these", "your", "his", "through", "don", "nor", "me", "were", "her", "more", "himself", "this", "down", "should", "our", "their", "while", "above", "both", "up", "to", "ours", "had", "she", "all", "no", "when", "at", "any", "before", "them", "same", "and", "been", "have", "in", "will", "on", "does", "yourselves", "then", "that", "because", "what", "over", "why", "so", "can", "did", "not", "now", "under", "he", "you", "herself", "has", "just", "where", "too", "only", "myself", "which", "those", "i", "after", "few", "whom", "t", "being", "if", "theirs", "my", "against", "a", "by", "doing", "it", "how", "further", "was", "here", "than"}
print(torch.__version__)
print(transformers.__version__)

torch.cuda.is_available()

#from google.colab import files
#uploaded = files.upload()
dataset = pd.read_csv("combined_age.tsv", delimiter='\t', header=None, names=['label', 'sentence'])
dataset.shape

def transformText(text, do_stop=False, do_stem=False):
    # Convert text to lower
    text = text.lower()
    
    # Cleaning input
    text = text.replace("'s","")
    text = text.replace("â€™s","")
    text = text.replace("?","")
    text = text.replace("-","")
    
    # Removing non ASCII chars    
    text = re.sub(r'[^\x00-\x7f]',r' ',text)
    # Strip multiple whitespaces
    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)
    # Removing all the stopwords
    if (do_stop==True):
        filtered_words = [word for word in text.split() if word not in stopwords]
    else:
        filtered_words = [word for word in text.split()]
    # Preprocessed text after stop words removal
    text = " ".join(filtered_words)
    # Remove the punctuation
    text = gensim.parsing.preprocessing.strip_punctuation2(text)
    # Strip multiple whitespaces
    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)
    if (do_stem==True):
        # Stemming
        text = gensim.parsing.preprocessing.stem_text(text)
    return text

dataset['preproc_text'] = dataset['sentence'].apply(lambda x: transformText(x, do_stop=False))
dataset.tail(5)

len(list(set(dataset.label)))

model_type = 'roberta'
## Distilbert
if model_type == 'roberta':
    print("RoBERTa")
    config = RobertaConfig.from_pretrained('roberta-large')
    config.num_labels = len(list(set(dataset.label)))
    config.num_hidden_layers = 4
    tokenizer = RobertaTokenizer.from_pretrained('roberta-large')
    model = RobertaForSequenceClassification(config)
print(config)

def prepare_features(seq_1, zero_pad = False, max_seq_length = 120):
    enc_text = tokenizer.encode_plus(seq_1, add_special_tokens=True, max_length=300)
    if zero_pad:
        while len(enc_text['input_ids']) < max_seq_length:
            enc_text['input_ids'].append(0)
            enc_text['token_type_ids'].append(0)
    return enc_text

class Intents(Dataset):
    def __init__(self, dataframe):
        self.len = len(dataframe)
        self.data = dataframe
        
    def __getitem__(self, index):
        utterance = self.data.preproc_text[index]
        X = prepare_features(utterance, zero_pad = True)
        y =  int(self.data.label[index])
        return np.array(X['input_ids']), np.array(X['token_type_ids']), np.array(y)
    
    def __len__(self):
        return self.len

train_size = 0.8
train_dataset=dataset.sample(frac=train_size,random_state=200).reset_index(drop=True)
test_dataset=dataset.drop(train_dataset.index).reset_index(drop=True)

training_set = Intents(train_dataset)
testing_set = Intents(test_dataset)

### Dataloaders Parameters
params = {'batch_size': 16,
          'shuffle': True,
          'drop_last': True,
          'num_workers': 0}
training_loader = DataLoader(training_set, **params)
testing_loader = DataLoader(testing_set, **params)
loss_function = nn.CrossEntropyLoss()
learning_rate = 1e-06
optimizer = optim.Adam(params =  model.parameters(), lr=learning_rate)
if torch.cuda.is_available():
    print("GPU is AVAILABLE!ðŸ¤˜ðŸ™ŒðŸ’ª")
    model = model.cuda()

ids, tokens, labels = next(iter(training_loader))
ids.shape, tokens.shape, labels

if model_type == 'roberta':
    print(model_type)
    out = model.forward(ids.cuda(), token_type_ids=tokens.cuda(), head_mask=None)[0]
print(loss_function(out, labels.cuda()))
print(out.shape)

@timebudget
def train(model, epochs):
  max_epochs = epochs
  model = model.train()
  for epoch in tqdm_notebook(range(max_epochs)):
      print("EPOCH -- {}".format(epoch))
      for i, (ids, tokens, labels) in enumerate(training_loader):
          optimizer.zero_grad()
          if torch.cuda.is_available():
              ids = ids.cuda()
              tokens = tokens.cuda()
              labels = labels.cuda()
          if model_type == 'roberta':
              output = model.forward(ids,token_type_ids=tokens)[0]
          loss = loss_function(output, labels)
          loss.backward()
          optimizer.step()
          if i%500 == 0:
              correct = 0
              total = 0
              for (ids, tokens, labels) in testing_loader:
                  if torch.cuda.is_available():
                      ids = ids.cuda()
                      tokens = tokens.cuda()
                      labels = labels.cuda()
                  if model_type == 'roberta':
                      output = model.forward(ids,token_type_ids=tokens)[0]
                  _, predicted = torch.max(output.data, 1)
                  total += labels.size(0)
                  correct += (predicted.cpu() == labels.cpu()).sum()
              print('Iteration: {}. Loss: {}'.format(i, loss.item()))
  return "Training finished!"

train(model, 100)

def get_reply_proba(msg, language = 'en'):
      model.eval()
      input_phrase = transformText(msg, do_stop=False,do_stem=False)
      features = prepare_features(input_phrase, zero_pad = True)
      ids = torch.tensor(features['input_ids']).unsqueeze(0)
      tokens = torch.tensor(features['token_type_ids']).unsqueeze(0)
      if torch.cuda.is_available():
          ids = ids.cuda()
          tokens = tokens.cuda()
      if model_type == 'distilbert':
        logits_out = model.forward(ids)[0].squeeze(0)
      else:
        logits_out = model.forward(ids,token_type_ids=tokens)[0].squeeze(0)
      softmax_out = F.softmax(logits_out, dim=0)
      _, pred_label = torch.max(softmax_out.data, 0)

      prediction=prediction=list(label_to_ix.keys())[list(label_to_ix.values()).index(pred_label.data.cpu())]
    
      return prediction

label_to_ix = {}
for label in dataset.label:
    if label not in label_to_ix:
        label_to_ix[label]=len(label_to_ix)
label_to_ix

#from google.colab import files
#uploaded = files.upload()
dataset = pd.read_csv("combined_age_test.tsv", delimiter='\t', header=None, names=['label', 'sentence'])
dataset.shape

dataset['preproc_text'] = dataset['sentence'].apply(lambda x: transformText(x, do_stop=False))
dataset.tail(5)

from google.colab import drive
drive.mount('/content/drive')

from sklearn.metrics import f1_score
act=[]
pred=[]
for index, row in dataset.iterrows():
    pred.append(get_reply_proba(row['preproc_text']))
    act.append(row['label'])
f1=f1_score(act, pred, average='micro')
dataset['pred'] = pred 
dataset.to_csv('/content/drive/My Drive/age.csv', index=False)
print(f1)
